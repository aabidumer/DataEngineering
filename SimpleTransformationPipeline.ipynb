{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBOSGKi/0ky3gWQPc22JH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aabidumer/DataEngineering/blob/master/SimpleTransformationPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT71yxSz2UUf",
        "outputId": "b3eec75b-3d1f-4320-c949-0170ae4be0f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=dada09e42358b9622bfc3614bb16edc53383b9ccac6e0ba548943df193cf30a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "logging\n",
        "~~~~~~~\n",
        "\n",
        "This module contains a class that wraps the log4j object instantiated\n",
        "by the active SparkContext, enabling Log4j logging for PySpark using.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Log4j(object):\n",
        "    \"\"\"Wrapper class for Log4j JVM object.\n",
        "\n",
        "    :param spark: SparkSession object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spark):\n",
        "        # get spark app details with which to prefix all messages\n",
        "        app_id = spark.sparkContext.getConf().get('spark.app.id')\n",
        "        app_name = spark.sparkContext.getConf().get('spark.app.name')\n",
        "\n",
        "        log4j = spark._jvm.org.apache.log4j\n",
        "        message_prefix = '<' + app_name + ' ' + app_id + '>'\n",
        "        self.logger = log4j.LogManager.getLogger(message_prefix)\n",
        "\n",
        "    def error(self, message):\n",
        "        \"\"\"Log an error.\n",
        "\n",
        "        :param: Error message to write to log\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.logger.error(message)\n",
        "        return None\n",
        "\n",
        "    def warn(self, message):\n",
        "        \"\"\"Log an warning.\n",
        "\n",
        "        :param: Error message to write to log\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.logger.warn(message)\n",
        "        return None\n",
        "\n",
        "    def info(self, message):\n",
        "        \"\"\"Log information.\n",
        "\n",
        "        :param: Information message to write to log\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        self.logger.info(message)\n",
        "        return None"
      ],
      "metadata": {
        "id": "1ngC5tl624nf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import __main__\n",
        "\n",
        "from os import environ, listdir, path\n",
        "from json import loads\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "\n",
        "def start_spark(app_name='my_spark_app', master='local[*]', jar_packages=[],\n",
        "                files=[], spark_config={}):\n",
        "    \"\"\"Start Spark session, get Spark logger and load config files.\n",
        "\n",
        "    Start a Spark session on the worker node and register the Spark\n",
        "    application with the cluster. Note, that only the app_name argument\n",
        "    will apply when this is called from a script sent to spark-submit.\n",
        "    All other arguments exist solely for testing the script from within\n",
        "    an interactive Python console.\n",
        "\n",
        "    This function also looks for a file ending in 'config.json' that\n",
        "    can be sent with the Spark job. If it is found, it is opened,\n",
        "    the contents parsed (assuming it contains valid JSON for the ETL job\n",
        "    configuration), into a dict of ETL job configuration parameters,\n",
        "    which are returned as the last element in the tuple returned by\n",
        "    this function. If the file cannot be found then the return tuple\n",
        "    only contains the Spark session and Spark logger objects and None\n",
        "    for config.\n",
        "\n",
        "    The function checks the enclosing environment to see if it is being\n",
        "    run from inside an interactive console session or from an\n",
        "    environment which has a `DEBUG` environment varibale set (e.g.\n",
        "    setting `DEBUG=1` as an environment variable as part of a debug\n",
        "    configuration within an IDE such as Visual Studio Code or PyCharm in\n",
        "    In this scenario, the function uses all available function arguments\n",
        "    to start a PySpark driver from the local PySpark package as opposed\n",
        "    to using the spark-submit and Spark cluster defaults. This will also\n",
        "    use local module imports, as opposed to those in the zip archive\n",
        "    sent to spark via the --py-files flag in spark-submit.\n",
        "\n",
        "    :param app_name: Name of Spark app.\n",
        "    :param master: Cluster connection details (defaults to local[*].\n",
        "    :param jar_packages: List of Spark JAR package names.\n",
        "    :param files: List of files to send to Spark cluster (master and\n",
        "        workers).\n",
        "    :param spark_config: Dictionary of config key-value pairs.\n",
        "    :return: A tuple of references to the Spark session, logger and\n",
        "        config dict (only if available).\n",
        "    \"\"\"\n",
        "\n",
        "    # detect execution environment\n",
        "    flag_repl = False if hasattr(__main__, '__file__') else True\n",
        "    flag_debug = True if 'DEBUG' in environ.keys() else False\n",
        "\n",
        "    if not (flag_repl or flag_debug):\n",
        "        # get Spark session factory\n",
        "        spark_builder = (\n",
        "            SparkSession\n",
        "            .builder\n",
        "            .appName(app_name))\n",
        "    else:\n",
        "        # get Spark session factory\n",
        "        spark_builder = (\n",
        "            SparkSession\n",
        "            .builder\n",
        "            .master(master)\n",
        "            .appName(app_name))\n",
        "\n",
        "        # create Spark JAR packages string\n",
        "        spark_jars_packages = ','.join(list(jar_packages))\n",
        "        spark_builder.config('spark.jars.packages', spark_jars_packages)\n",
        "\n",
        "        spark_files = ','.join(list(files))\n",
        "        spark_builder.config('spark.files', spark_files)\n",
        "\n",
        "        # add other config params\n",
        "        for key, val in spark_config.items():\n",
        "            spark_builder.config(key, val)\n",
        "\n",
        "    # create session and retrieve Spark logger object\n",
        "    spark_sess = spark_builder.getOrCreate()\n",
        "    spark_logger = Log4j(spark_sess)\n",
        "\n",
        "    # get config file if sent to cluster with --files\n",
        "    spark_files_dir = SparkFiles.getRootDirectory()\n",
        "    config_files = [filename\n",
        "                    for filename in listdir(spark_files_dir)\n",
        "                    if filename.endswith('config.json')]\n",
        "\n",
        "    if len(config_files) != 0:\n",
        "        path_to_config_file = path.join(spark_files_dir, config_files[0])\n",
        "        with open(path_to_config_file, 'r') as config_file:\n",
        "            config_json = config_file.read().replace('\\n', '')\n",
        "        config_dict = loads(config_json)\n",
        "        spark_logger.warn('loaded config from ' + config_files[0])\n",
        "    else:\n",
        "        spark_logger.warn('no config file found')\n",
        "        config_dict = None\n",
        "\n",
        "    return spark_sess, spark_logger, config_dict"
      ],
      "metadata": {
        "id": "bj3wOu4y2zRy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZF-JcWQ2LTt"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, concat_ws, lit\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main ETL script definition.\n",
        "\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "       \n",
        "    # start Spark application and get Spark session, logger and config\n",
        "    spark, log, config = start_spark(\n",
        "        app_name='my_etl_job',\n",
        "        files=[\"/content/etl_config.json\"])\n",
        "\n",
        "    # log that main ETL job is starting\n",
        "    log.warn('etl_job is up-and-running')\n",
        "\n",
        "    # execute ETL pipeline\n",
        "    data = extract_data(spark)\n",
        "    data_transformed = transform_data(data, config['steps_per_floor'])\n",
        "    load_data(data_transformed)\n",
        "\n",
        "    # log the success and terminate Spark application\n",
        "    log.warn('test_etl_job is finished')\n",
        "    spark.stop()\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_data(spark):\n",
        "    \"\"\"Load data from Parquet file format.\n",
        "\n",
        "    :param spark: Spark session object.\n",
        "    :return: Spark DataFrame.\n",
        "    \"\"\"\n",
        "    df = (\n",
        "        spark\n",
        "        .read\n",
        "        .parquet('tests/test_data/employees'))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def transform_data(df, steps_per_floor_):\n",
        "    \"\"\"Transform original dataset.\n",
        "\n",
        "    :param df: Input DataFrame.\n",
        "    :param steps_per_floor_: The number of steps per-floor at 43 Tanner\n",
        "        Street.\n",
        "    :return: Transformed DataFrame.\n",
        "    \"\"\"\n",
        "    df_transformed = (\n",
        "        df\n",
        "        .select(\n",
        "            col('id'),\n",
        "            concat_ws(\n",
        "                ' ',\n",
        "                col('first_name'),\n",
        "                col('second_name')).alias('name'),\n",
        "               (col('floor') * lit(steps_per_floor_)).alias('steps_to_desk')))\n",
        "\n",
        "    return df_transformed\n",
        "\n",
        "\n",
        "def load_data(df):\n",
        "    \"\"\"Collect data locally and write to CSV.\n",
        "\n",
        "    :param df: DataFrame to print.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    (df\n",
        "     .coalesce(1)\n",
        "     .write\n",
        "     .csv('loaded_data', mode='overwrite', header=True))\n",
        "    return None\n",
        "\n",
        "\n",
        "def create_test_data(spark, config):\n",
        "    \"\"\"Create test data.\n",
        "\n",
        "    This function creates both both pre- and post- transformation data\n",
        "    saved as Parquet files in tests/test_data. This will be used for\n",
        "    unit tests as well as to load as part of the example ETL job.\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # create example data from scratch\n",
        "    local_records = [\n",
        "        Row(id=1, first_name='Dan', second_name='Germain', floor=1),\n",
        "        Row(id=2, first_name='Dan', second_name='Sommerville', floor=1),\n",
        "        Row(id=3, first_name='Alex', second_name='Ioannides', floor=2),\n",
        "        Row(id=4, first_name='Ken', second_name='Lai', floor=2),\n",
        "        Row(id=5, first_name='Stu', second_name='White', floor=3),\n",
        "        Row(id=6, first_name='Mark', second_name='Sweeting', floor=3),\n",
        "        Row(id=7, first_name='Phil', second_name='Bird', floor=4),\n",
        "        Row(id=8, first_name='Kim', second_name='Suter', floor=4)\n",
        "    ]\n",
        "\n",
        "    df = spark.createDataFrame(local_records)\n",
        "\n",
        "    # write to Parquet file format\n",
        "    (df\n",
        "     .coalesce(1)\n",
        "     .write\n",
        "     .parquet('tests/test_data/employees', mode='overwrite'))\n",
        "\n",
        "    # create transformed version of data\n",
        "    df_tf = transform_data(df, config['steps_per_floor'])\n",
        "\n",
        "    # write transformed version of data to Parquet\n",
        "    (df_tf\n",
        "     .coalesce(1)\n",
        "     .write\n",
        "     .parquet('tests/test_data/employees_report', mode='overwrite'))\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# entry point for PySpark ETL application\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rl7FsRPg2OYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}